VZLUSAT has a much more noisy behaviour, and that is not something that reflect the actual link state, but is something that is related to how the signal power is calculated.

![alt text](screenshots/image-77.png)
My goal is that it should the yellow lines for the whole thing

There are some outliers
![alt text](screenshots/image-76.png)

Interference
![alt text](screenshots/image-78.png)
![alt text](screenshots/image-79.png)


More like that the noise power is affected by the clean signal power
![alt text](screenshots/image-80.png)


![alt text](screenshots/image-81.png)


There are some weird things
![alt text](screenshots/image-82.png)



I dont know why this one looks like shit

Finer resolution
![alt text](screenshots/image-83.png)

Still very very noisy data. I am guessing the only smart solution is making a moving average on the data. Then it will be called the truth that a model will have to predict. It is not completely stupid, but not that sexy.


Checking how the outliers look in the waterfall plot
![alt text](screenshots/image-84.png)
![alt text](screenshots/image-85.png)
Only two that are very fucked, and should be discarded


I do not like the noise:

Rectangular window
![alt text](screenshots/image-87.png)

Hamming windowed moving average
![alt text](screenshots/image-86.png)
larger window
![alt text](screenshots/image-88.png)

time resolution of 0.01 and fft size 256
![alt text](screenshots/image-89.png)

The red line looks more like a real signal. I am going to make it crazier. it was not that different unfortunately

I reduced the resolution to 128 and time to 0.01
![alt text](screenshots/image-90.png)

I can now see the noise. And the inteference. How large can the doppler shift be? it is never larger than 50k

FD=(v/c)*f
v=7600km/s max
c=3*10^8 m/s
f=437MHz

Max doppler= 11kHz
sampling rate is 250kHz
fft resolution is obj_act[].noise_obj.fft_size

If max value of the fft in valid range is not 10 times larger than the obj_act.noise, the data is an outlier and should be removed.


The variance cannot directly be used for anything unfortunately. unless maybe no, even normalising is not doing anything....

the noise could probably also be found by a histogram instead of just the average of specific bins

![alt text](screenshots/image-91.png)
larger
![alt text](screenshots/image-92.png)

not that much was gained from going to 0.005, but 0.01 and 32 are my recommended values

LPC of order 1000 (basically AR(1000)) where weights were found from 1 pass
![alt text](screenshots/image-93.png)
Not too bad for such a simple model
![alt text](screenshots/image-94.png)




When doing Machine Learning, each point should basically work by itself. This means each point should have the information about the previous points if that information is usefull

I tried to make chatten provide new ideas for features that could be calculated for each value
Statistical Features
Statistical measures can capture variations, trends, and spread in the data.

# Rolling Statistics (e.g., Last 5 Seconds)

Mean (already implemented)
Standard deviation
Variance
Median
Range (max - min)
Interquartile range (IQR)
Global Trends

Cumulative sum
Cumulative mean
Skewness and Kurtosis

Skewness: Measures data symmetry.
Kurtosis: Measures whether data tails are heavy or light compared to a normal distribution.
Change Metrics

Rate of change: Difference between consecutive values.
Percentage change: Relative rate of change compared to the previous value.
# Frequency-Domain Features
Signal features in the frequency domain often highlight periodicity or noise.

Power Spectral Density (PSD)

Quantifies the energy present at various frequency components.
Useful for identifying dominant frequencies in the signal.
Dominant Frequency

The frequency component with the highest amplitude in the Fourier Transform.
Harmonics

The presence and strength of harmonics in the frequency spectrum.
Signal Energy

Calculated as the sum of squared values over a window.
# Time-Series Decomposition Features
Decompose your signal into components for feature extraction.

Trend

The underlying long-term progression in the data (e.g., smoothed using moving averages or polynomial fitting).
Seasonality

Extract periodic patterns (daily, weekly, etc.) using methods like STL decomposition.
Residual

The remaining part of the signal after removing trend and seasonality.
Outlier and Event Detection
Outlier Count

Number of outliers in the last 5 seconds (e.g., using thresholds or Z-scores).
Amplitude Threshold

Count of values exceeding a certain threshold (high or low).
Event Durations

Duration of spikes, dips, or flat regions in the signal.
# Signal Quality Features
Features that measure the reliability or noise level of the signal.

Signal-to-Noise Ratio (SNR)

Ratio of signal power to noise power.
Noise Variance

Variance of the noise component.
Entropy

Shannon entropy: Measures the uncertainty or randomness in the signal.
Signal Smoothness

Metrics like Mean Absolute Difference (MAD) between consecutive samples.
# Correlation Features
If multiple signals are available, cross-correlation can capture relationships.

Cross-Correlation

Correlation between clean_sig_abs and other signals (e.g., pointing error, noise).
Lagged Correlation

Correlation between a signal and itself at different time lags.
# Event-Based Features
Peak Detection

Count of peaks or troughs in the last 5 seconds.
Amplitude of the highest peak.
Peak-to-Peak Interval

Time between consecutive peaks.
Peak Symmetry

Ratio of positive to negative peaks.
Time and Context Features
Time of Day

Include the timestamp as a feature (e.g., hour of the day or day of the week) if patterns vary by time.
Station Characteristics

Include contextual information such as station_obj.dist, azimuth, or elevation to help the model.
# Advanced Features
Autoregressive Coefficients

Fit an AR or ARIMA model to the signal and use the coefficients as features.
Wavelet Transform Coefficients

Perform wavelet decomposition to capture both time and frequency characteristics.
Dynamic Time Warping (DTW) Distance

Compare the current signal window to a template or reference signal.
Lyapunov Exponent

Measures the chaos in the signal.
Fractal Dimension

Quantifies complexity or self-similarity in the signal.


But if i want to predict 10 seconds into the future, all the values except time, set_azimuth, set_elevation, distance are unknown (as these are deterministic and the others are measured) for the data point. The mean of the last 5 seconds must then be the mean of the 15 to 10 seconds old samples.

In practice it also do not make sense to calculate the signal quality 10 seconds into the future 100 times a second. However it is also nice to have a reasonable amount of data for regression for example. It could be just be a longer time span and then weigh the samples so the newest samples are the most important.

What about the time aspect?
Basically the slope for the regression part can have the slope be in time. Maybe the slope is calculated from 5 or 1000 points, Maybe this value could also be appended? In that way it can find out how much weight should be used for the prediction.
what about the start points?

Tried adding features to each point so it is not dependant on the previous indicies values
![alt text](screenshots/image-95.png)
![alt text](screenshots/image-96.png)

What we can see is that if the points are low or the MSE is high, the prediction will be shit, and we therefore cannot trust it. This could be added for a second degree polynomial. If it does not fit the data, dont use it in the model, easy as that. We are going to get so many features